---
title: "data_607_assignment_10"
author: "Maxfield Raynolds"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The following code block loads packages.

```{r packages and sentiment library}
library(tidytext)
library(tidyverse)
library(ggplot2)
library(wordcloud)
library(reshape2)
library(xml2)

get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
```

The following code block loads Lewis Carroll books from the gutenberg project website.

```{r load specific lewis carroll books from the gutenberg project website}
#The selected titles were not available from the gutenberg project package so the following code imports them from the gutenberg project website

wonderland <- as.data.frame(read_lines("https://www.gutenberg.org/cache/epub/11/pg11.txt"))

wonderland <- wonderland |> mutate(
  book = "Alice's Adventures in Wonderland")

colnames(wonderland)[1] = "text"

looking_glass <- as.data.frame(read_lines("https://www.gutenberg.org/cache/epub/12/pg12.txt"))

looking_glass <- looking_glass |> mutate(
  book = "Through the Looking-Glass"
)

colnames(looking_glass)[1] = "text"

carroll_raw <- wonderland |> bind_rows(looking_glass)
```

## Adapted Sentiment Analysis

Much of the following code is adapted from "Text Mining with R: A Tidy Approach" by Silge & Robinson, available at <https://www.tidytextmining.com/>

The following code creates a tidy dataframe for the Lewis Carroll texts.

```{r tidy_carroll}
tidy_carroll <- carroll_raw |> 
  group_by(book) |> 
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text,
                                regex("(?i)^chapter [\\divxlc]\\.")))) |> 
  ungroup()

tidy_carroll <- unnest_tokens(tidy_carroll,word,text)
```

The following code filters the terms labelled for "joy" from the nrc sentiment library.

```{r filter words labeled with joy from nrc sentiment library, count joy words in Emma}
nrc_joy <- get_sentiments("nrc") |> 
  filter(sentiment == "joy")

tidy_carroll |> 
  filter(book == "Alice's Adventures in Wonderland") |> 
  inner_join(nrc_joy, by = join_by(word)) |> 
  count(word, sort = TRUE)
```

The following code analyzes the sentiment for the Lewis Carroll texts using the "bing" library in 80 line sections.

```{r categorize the sentiment for lewis carroll books using bing library in 80 line sections}
lewis_carroll_sentiment <- tidy_carroll |> 
  inner_join(get_sentiments("bing")) |> 
  count(book, index = linenumber %/% 80, sentiment) |> 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |> 
  mutate(sentiment = positive - negative)
```

The following code block plots the sentiment of the two Lewis Carroll texts over the course of the book using the "bing" sentiment library.

```{r plot of sentiment of lewis carrol books}
ggplot(lewis_carroll_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

The following code analyzes sentiments using the three sentiment lexicons for Alice's Adventures in Wonderland: afinn, bing, and nrc.

```{r compare sentiment dictionaries}
alice_wonderland <- tidy_carroll |> 
  filter(book == "Alice's Adventures in Wonderland")

afinn <- alice_wonderland |> 
  inner_join(get_sentiments("afinn")) |> 
  group_by(index = linenumber %/% 80) |> 
  summarise(sentiment = sum(value)) |> 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  alice_wonderland |> 
    inner_join(get_sentiments("bing")) |> 
    mutate(method = "Bing et al."),
  alice_wonderland |> 
  inner_join(get_sentiments("nrc") |> 
               filter(sentiment %in% c("positive",
                                       "negative"))) |> 
    mutate(method = "NRC")) |> 
  count(method, index = linenumber %/% 80, sentiment) |> 
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) |> 
  mutate(sentiment = positive - negative)
```

The following code plots the sentiment for each of the lexicons' analysis of Alice's Adventures in Wonderland. As can be seen NRC has an overall more positive sentiment but all three show a similar fluctuation in sentiment.

```{r estimate net sentiment in each section}
bind_rows(afinn, bing_and_nrc) |> 
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

The counts below show the differences in negative vs. positive word counts in the nrc and bing lexicons.

```{r count of sentiment words in nrc}
get_sentiments("nrc") |> 
  filter(sentiment %in% c("positive", "negative")) |> 
  count(sentiment)
```

```{r count of sentiment words in bing}
get_sentiments("bing") |> 
  count(sentiment)
```

The following code shows the most common words in Lewis Carroll's texts.

```{r most common words in Lews Carroll}
bing_word_counts <- tidy_carroll |> 
  inner_join(get_sentiments("bing")) |> 
  count(word, sentiment, sort = TRUE) |> 
  ungroup() |> print()
```

The following codes plots the most common positive and negative words in the Lewis Carroll's texts.

```{r plot most common words by sentiment}
bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

```{r create custom stop words library}
custom_stop_words <- bind_rows(tibble(word = c("gutenberg"),  
                                      lexicon = c("custom")), 
                               stop_words)
```

The following code creates word clouds from the Carroll texts.

```{r word cloud}
tidy_carroll |> 
  anti_join(custom_stop_words) |> 
  count(word) |> 
  with(wordcloud(word, n, max.words = 100))
```

The following code creates a word cloud of the most common positive and negative sentiment words in the Lewis Carroll works.

```{r reshape word cloud}
tidy_carroll %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray80", "gray20"),
                   max.words = 100)
```

## Original Sentiment Analysis

The following code uses the sentimentr package to analyze the Lewis Carroll text by sentence.

This loads the sentimentr package

```{r load sentimentr}
library(sentimentr)
```

The following code loads the Lewis Carroll texts, separates them by sentence and combines them into a dataframe for analysis. This dataframe is, for these purposes, is tidy.

```{r organize and tidy the lewis carroll text into sentences}
wonder <- read_lines("https://www.gutenberg.org/cache/epub/11/pg11.txt")

wonder <- str_c(wonder, collapse = " ")

wonder <- as.data.frame(wonder, nm = "text") |> unnest_tokens(sentence, text, token = "sentences") |>  separate_longer_delim(sentence, delim = "chapter") |> mutate(
  sentence = str_replace_all(sentence,"\\s+([ivxlc]+)","chapter \\1"),
  sentence = str_replace_all(sentence, "([a-z\\W_])chapter", "\\1 "),
  book = "Alice's Adventures in Wonderland") |> 
  filter(row_number() > 29) |> 
  mutate(chapter = cumsum(str_detect(sentence,
                                regex("^chapter")))) |> 
  group_by(chapter) |> 
  mutate(sentence_no = row_number()) |> ungroup()

glass <- read_lines("https://www.gutenberg.org/cache/epub/12/pg12.txt")

glass <- str_c(glass, collapse = " ")

glass <- as.data.frame(glass, nm = "text") |> unnest_tokens(sentence, text, token = "sentences") |>  separate_longer_delim(sentence, delim = "chapter") |> mutate(
  sentence = str_replace_all(sentence,"\\s+([ivxlc]+)","chapter \\1"),
  sentence = str_replace_all(sentence, "([a-z\\W_])chapter", "\\1 "),
  book = "Through the Looking-Glass") |> 
  filter(row_number() > 129) |> 
  mutate(chapter = cumsum(str_detect(sentence,
                                regex("^chapter")))) |> 
  group_by(chapter) |> 
  mutate(sentence_no = row_number()) |> ungroup()

carroll_sentences <- wonder |> bind_rows(glass)
```

The following code uses sentimentr to analyze the sentiment of the Lewis Carroll works by chapter.

```{r sentiment analysis by chapter wtih sentimentr}
library(magrittr)

carroll_sentiment_chapter <- carroll_sentences |> 
  get_sentences() %$% 
  sentiment_by(sentence, list(book, chapter))

head(carroll_sentiment_chapter)
```

The following plot shows the average sentiment throughout the two books by chapter.

```{r }
 ggplot(carroll_sentiment_chapter, aes(chapter, ave_sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 1, scales = "free_y")
```


### Some updates (Farhod Ibragimov).Created the function for computing sentiment over chunks of text using three lexicons—AFINN, Bing, and NRC—and then applies it to a tidy text dataset

```{r}

lexicons <- list(
  afinn = get_sentiments("afinn"),
  bing  = get_sentiments("bing"),
  nrc   = get_sentiments("nrc") |> 
            filter(sentiment %in% c("positive","negative"))
)
names(tidy_carroll)
compute_sentiment <- function(data, lex, chunk_size = 80, keep_value = FALSE) {
  df <- data %>% inner_join(lex, by = "word")
  
  if (keep_value) {
    df |>
      group_by(index = linenumber %/% chunk_size) |>
      summarise(sentiment = sum(value, na.rm = TRUE))
  } else {
    df |>
      count(index = linenumber %/% chunk_size, sentiment) |>
      pivot_wider(names_from   = sentiment,
                  values_from  = n,
                  values_fill  = 0) |>
      mutate(sentiment = positive - negative)
  }
}

afinn_df <- compute_sentiment(tidy_carroll, lexicons$afinn, keep_value = TRUE)
bing_df  <- compute_sentiment(tidy_carroll, lexicons$bing)
nrc_df   <- compute_sentiment(tidy_carroll, lexicons$nrc)

head(afinn_df)
head(bing_df)
head(nrc_df)

```

Code pulls each sentiment table once into a lexicons list (and pre-filter NRC), instead of calling `get_sentiments()` repeatedly.

Function `compute_sentiment()` does both AFINN (summing values) and binary lexicons (counting pos/neg) via a straightforward if/else with %\>%.
